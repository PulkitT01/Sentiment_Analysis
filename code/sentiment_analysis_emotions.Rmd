---
title: "Sentiment Analysis"
author: "Pulkit"
date: "2024-03-14"
output: html_document
---

---
title: Sentiment Analysis on Emotions Dataset
author: Pulkit Thukral
date: \today
output:
  html_document:
    theme: cerulean
---

The details below about the dataset can be found on [https://www.kaggle.com/datasets/nelgiriyewithana/emotions](https://www.kaggle.com/datasets/nelgiriyewithana/emotions).


# Introduction

Welcome to the "Emotions" dataset â€“ a collection of English Twitter messages meticulously annotated with six fundamental emotions: anger, fear, joy, love, sadness, and surprise. This dataset serves as a valuable resource for understanding and analyzing the diverse spectrum of emotions expressed in short-form text on social media.

## About the Dataset

Each entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within the realm of social media.

## Key Features

- **text**: A string feature representing the content of the Twitter message.
- **label**: A classification label indicating the primary emotion, with values ranging from 0 to 5.

## Sample Data

Here's a glimpse of the dataset with a few examples:

```{r sample_data, echo=FALSE}
# Import the data
data<- read.csv("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/data/text.csv")

# View the first few rows to see what the data is like
head(data)
```

## Load the necessary libraries
```{r load_libraries, echo=TRUE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(stringr)
library(tm)
```

### Use searchpaths() to see all the currently loaded packages
```{r searchpaths, echo=TRUE}
searchpaths()
```

The output of `searchpaths()` reveals all paths searched by R to locate functions and data. It includes explicitly loaded libraries, implicit dependencies, system directories, autoloads, and RStudio tools, ensuring accessibility of required resources.


### Summary of the data
```{r summary, echo=FALSE}
summary(data)
```

### Check for missing values
```{r missing_values, echo=TRUE}
sum(is.na(data))
```

## Distribution of labels in the data:
```{r, echo=FALSE}
table(data$label)
```

```{r labels_distribution, echo=FALSE}
# Defining a mapping from labels to emotions
label_to_emotion<- c("sadness", "joy", "love", "anger", "fear", "surprise")

# Create a new column "emotion" by mapping "label" column to emotions
data$emotion<- label_to_emotion[data$label+1]

# Plotting the histogram of emotions
plot <- ggplot(data, aes(x=emotion)) +
  geom_histogram(stat="count") +
  xlab("Emotion") +
  ylab("Count") +
  ggtitle("Histogram of Emotions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, color = "black"), 
        plot.background = element_rect(fill = "white"))  
plot
```


```{r saving_figure1, echo=FALSE}
# Saving this plot in figures folder
ggsave("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/figures/histogram_emotions.png", plot, width = 8, height = 6, units = "in")
```


## Preprocessing to the data

### Create a corpus
```{r corpus, echo=TRUE}
corpus_outside <- Corpus(VectorSource(data$text))
```

### Function to do the preprocessing
```{r preprocessing, echo=TRUE}
preprocesstext <- function(corpus){
  # Unnecessary variable created inside the function
  unnecessary_var <- "This is unnecessary"
  
  # Save the environment inside the function
  env_inside <- environment()
  
  # Convert text to lowercase
  corpus <- tm_map(corpus, tolower)
  
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  
  # Remove stopwords from the corpus
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  # Apply stemming transformation to the corpus
  corpus <- tm_map(corpus, stemDocument)
  
  return(list(corpus = corpus, env_inside = env_inside))
}

```


### Call the function and save the outputs
```{r function_call, echo=TRUE, warning=FALSE, message=FALSE}
output <- preprocesstext(corpus_outside)
corpus <- output$corpus
env_inside <- output$env_inside
```

### Checking environments inside and outside the function
```{r environments, echo=FALSE}
print("Contents of the environment outside the function:")
print(ls(envir = environment()))

print("Contents of the environment inside the function:")
print(ls(envir = env_inside))
```

When printing the contents of the environment outside the function, I observe a list of variables including "corpus", "cleaned_text", "plot", and others. This environment captures the variables defined in the global scope.

When inspecting the environment inside the function, I notice fewer variables compared to the global environment. This environment includes "corpus", "env_inside", and an additional variable named "unnecessary_var". This variable was created within the function but is not needed outside, demonstrating the scoping and encapsulation of variables within functions.


### Checking memory address of corpus inside and outside the function
```{r corpus_memory, echo=FALSE}
corpus_inside <- corpus

# Check if the corpus variable inside and outside the function points to the same memory address
cat("Memory address of corpus outside the function:", tracemem(corpus_outside), "\n")
cat("Memory address of corpus inside the function:", tracemem(corpus_inside), "\n")
```

The memory addresses for corpus_outside and corpus_inside differ because each variable exists within its respective scope. Variables declared outside the function (corpus_outside) and those created within it (corpus_inside) occupy distinct memory locations due to the scoping rules in R. As a result, changes made to corpus_inside within the function do not affect corpus_outside outside the function's scope. This encapsulation principle ensures that modifications made within a function remain isolated, reflecting the notion: "What happens in the function stays in the function."


### Convert corpus back to character vector
```{r cleaned_text, echo=FALSE}
cleaned_text <- sapply(corpus, as.character)

# Adding the cleaned_text back to the original dataframe
data$cleaned_text <- cleaned_text

# View the head of the data frame with the new cleaned_text column
head(data)
```

```{r saving_file, echo=FALSE}
# Save the preprocessed data to a CSV file
write.csv(data, file = "C:/Users/pulki/Documents/R projects/Sentiment_Analysis/data/preprocessed_data.csv", row.names = FALSE)
```


### Benchmark sapply() and lapply()
```{r benchmark, echo=FALSE}
# Define the functions to be benchmarked
f_sapply <- function(corpus) {
  sapply(corpus, as.character)
}

f_lapply <- function(corpus) {
  unlist(lapply(corpus, as.character))
}

# Perform the benchmarking
bench <- microbenchmark(
  sapply = f_sapply(corpus),
  lapply = f_lapply(corpus),
  times = 100
)

# Plot the results
autoplot(bench)
```


```{r saving_figure2, echo=FALSE}
ggsave("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/figures/benchmark_plot.png", autoplot(bench), width = 8, height = 6, units = "in")
```

The graph compares the performance of *lapply* and *sapply* in R programming over time. I observe that both functions have similar performance characteristics for the task being measured. This is indicated by the nearly identical shapes of their plots over the specified time interval.

This leads me to infer that for this specific task, there isn't a significant difference in performance between *lapply* and *sapply*. Therefore, I would conclude that the choice between these two functions can be made based on other factors such as code readability or the structure of the output, rather than performance. 

However, I would like to note that this is a specific case, and performance may vary depending on the nature of the task and the data being processed. It's always a good practice to benchmark your own code when performance is a critical factor.



