---
title: "Sentiment Analysis"
author: "Pulkit"
date: "2024-03-14"
output:
  html_document:
    code_folding: hide
---


The details below about the dataset can be found on [https://www.kaggle.com/datasets/nelgiriyewithana/emotions](https://www.kaggle.com/datasets/nelgiriyewithana/emotions).


# Introduction

Welcome to the "Emotions" dataset â€“ a collection of English Twitter messages meticulously annotated with six fundamental emotions: anger, fear, joy, love, sadness, and surprise. This dataset serves as a valuable resource for understanding and analyzing the diverse spectrum of emotions expressed in short-form text on social media.

## About the Dataset

Each entry in this dataset consists of a text segment representing a Twitter message and a corresponding label indicating the predominant emotion conveyed. The emotions are classified into six categories: sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5). Whether you're interested in sentiment analysis, emotion classification, or text mining, this dataset provides a rich foundation for exploring the nuanced emotional landscape within the realm of social media.

## Key Features

- **text**: A string feature representing the content of the Twitter message.
- **label**: A classification label indicating the primary emotion, with values ranging from 0 to 5.

## Load the necessary libraries
```{r load_libraries, warning=FALSE, message=FALSE}
library(knitr)
library(microbenchmark)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(stringr)
library(tm)
library(text2vec)
library(caTools)
library(xgboost)
library(caret)
```

### Instances where the libraries are used:

**knitr**:
- Used for printing data using `kable()`.

**microbenchmark**:
- Used for benchmarking `sapply()` and `lapply()` functions.

**tidyverse**:
- Used for data manipulation and visualization.
- Specifically, functions from the `ggplot2` package were used for plotting histograms and creating visualizations.

**tidytext**:
- Used for text preprocessing tasks.
- Functions like `unnest_tokens()` and `count()` are commonly used for tokenizing and counting words in text data.

**ggplot2**:
- Used for creating visualizations, such as histograms.

**stringr**:
- Used for string manipulation tasks.
- Functions like `str_replace()` are commonly used for replacing patterns in strings.

**tm**:
- Used for text mining tasks.
- Functions like `Corpus()` and `tm_map()` are used for creating a corpus and performing text preprocessing tasks.

**text2vec**:
- Used for converting text data into numerical vectors.
- Functions like `itoken()`, `create_vocabulary()`, and `create_dtm()` are used for text vectorization.

**caTools**:
- Used for data splitting.
- Specifically, the `sample.split()` function is used for splitting data into training and testing sets.

**xgboost**:
- Used for building and training the XGBoost model.
- Functions like `xgb.train()` and `xgb.DMatrix()` are used for model training and prediction.


## Sample Data

Here's a glimpse of the dataset with a few examples:

```{r sample_data}
# Import the data
data<- read.csv("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/data/text.csv")

# Print the data using kable()
kable(head(data), format = "markdown", row.names = FALSE)
```

### Use searchpaths() to see all the currently loaded packages
```{r searchpaths}
searchpaths()
```

The output of `searchpaths()` reveals all paths searched by R to locate functions and data. It includes explicitly loaded libraries, implicit dependencies, system directories, autoloads, and RStudio tools, ensuring accessibility of required resources.


### Summary of the data
```{r summary}
summary(data)
```

### Check for missing values
```{r missing_values}
sum(is.na(data))
```

## Distribution of labels in the data:
```{r}
table(data$label)
```

```{r labels_distribution}
# Defining a mapping from labels to emotions
label_to_emotion<- c("sadness", "joy", "love", "anger", "fear", "surprise")

# Create a new column "emotion" by mapping "label" column to emotions
data$emotion<- label_to_emotion[data$label+1]

# Plotting the histogram of emotions
plot <- ggplot(data, aes(x=emotion)) +
  geom_histogram(stat="count") +
  xlab("Emotion") +
  ylab("Count") +
  ggtitle("Histogram of Emotions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, color = "black"), 
        plot.background = element_rect(fill = "white"))  
plot

# Saving this plot in figures folder
ggsave("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/figures/histogram_emotions.png", plot, width = 8, height = 6, units = "in")
```

## Text data preprocessing pipeline

### Create a corpus
```{r corpus}
corpus_outside <- Corpus(VectorSource(data$text))
```

### Function to do the preprocessing
```{r preprocessing}
preprocesstext <- function(corpus){
  # Unnecessary variable created inside the function
  unnecessary_var <- "This is unnecessary"
  
  # Save the environment inside the function
  env_inside <- environment()
  
  # Convert text to lowercase
  corpus <- tm_map(corpus, tolower)
  
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  
  # Remove stopwords from the corpus
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  # Apply stemming transformation to the corpus
  corpus <- tm_map(corpus, stemDocument)
  
  return(list(corpus = corpus, env_inside = env_inside))
}

```


### Call the function and save the outputs
```{r function_call, warning=FALSE, message=FALSE}
output <- preprocesstext(corpus_outside)
corpus <- output$corpus
env_inside <- output$env_inside
```

### Data after preprocessing
```{r cleaned_text}
cleaned_text <- sapply(corpus, as.character)

# Adding the cleaned_text back to the original dataframe
data$cleaned_text <- cleaned_text

# View the head of the data frame with the new cleaned_text column
kable(head(data), format = "markdown", row.names = FALSE)

# Save the preprocessed data to a CSV file
write.csv(data, file = "C:/Users/pulki/Documents/R projects/Sentiment_Analysis/data/preprocessed_data.csv", row.names = FALSE)
```

### Checking environments inside and outside the function
```{r environments}
print("Contents of the environment outside the function:")
print(ls(envir = environment()))

print("Contents of the environment inside the function:")
print(ls(envir = env_inside))
```

When printing the contents of the environment outside the function, I observe a list of variables including "corpus", "cleaned_text", "plot", and others. This environment captures the variables defined in the global scope.

When inspecting the environment inside the function, I notice fewer variables compared to the global environment. This environment includes "corpus", "env_inside", and an additional variable named "unnecessary_var". This variable was created within the function but is not needed outside, demonstrating the scoping and encapsulation of variables within functions.


### Checking memory address of corpus inside and outside the function
```{r corpus_memory}
corpus_inside <- corpus

# Check if the corpus variable inside and outside the function points to the same memory address
cat("Memory address of corpus outside the function:", tracemem(corpus_outside), "\n")
cat("Memory address of corpus inside the function:", tracemem(corpus_inside), "\n")
```

The memory addresses for corpus_outside and corpus_inside differ because each variable exists within its respective scope. Variables declared outside the function (corpus_outside) and those created within it (corpus_inside) occupy distinct memory locations due to the scoping rules in R. As a result, changes made to corpus_inside within the function do not affect corpus_outside outside the function's scope. This encapsulation principle ensures that modifications made within a function remain isolated, reflecting the notion: "What happens in the function stays in the function."

### Benchmark sapply() and lapply()
```{r benchmark}
# Define the functions to be benchmarked
f_sapply <- function(corpus) {
  sapply(corpus, as.character)
}

f_lapply <- function(corpus) {
  unlist(lapply(corpus, as.character))
}

# Perform the benchmarking
bench <- microbenchmark(
  sapply = f_sapply(corpus),
  lapply = f_lapply(corpus),
  times = 100
)

# Plot the results
autoplot(bench)
```


```{r saving_figure2}
ggsave("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/figures/benchmark_plot.png", autoplot(bench), width = 8, height = 6, units = "in")
```

The graph compares the performance of *lapply* and *sapply* in R programming over time. I observe that both functions have similar performance characteristics for the task being measured. This is indicated by the nearly identical shapes of their plots over the specified time interval.

This leads me to infer that for this specific task, there isn't a significant difference in performance between *lapply* and *sapply*. Therefore, I would conclude that the choice between these two functions can be made based on other factors such as code readability or the structure of the output, rather than performance. 

However, I would like to note that this is a specific case, and performance may vary depending on the nature of the task and the data being processed. It's always a good practice to benchmark your own code when performance is a critical factor.


# Machine Learning with XGBoost

### Loading Preprocessed Data
```{r}
data1<- read.csv("C:/Users/pulki/Documents/R projects/Sentiment_Analysis/data/preprocessed_data.csv")
```

### Split the data into features (X) and target (y)
```{r}
X <- data1$cleaned_text
y <- data1$label
```

### Convert the text data into numerical vectors
```{r text_to_numeric}
it <- itoken(X, progressbar = FALSE)
v <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(v)
dtm <- create_dtm(it, vectorizer)
```

### Split the data into training and testing sets
```{r, data_split}
set.seed(123)
split <- sample.split(y, SplitRatio = 0.8)
X_train <- dtm[split, ]
y_train <- y[split]
X_test <- dtm[!split, ]
y_test <- y[!split]
```

### Size of training and test sets
```{r}
cat("Training set:", dim(X_train), ", y_train:", length(y_train), "\n")
cat("Testing set:", dim(X_test), ", y_test:", length(y_test), "\n")
```
## XGBoost Model Training and Evaluation

### Convert the sparse matrix to a format xgboost can handle
```{r}
dtrain <- xgb.DMatrix(X_train, label = y_train)
dtest <- xgb.DMatrix(X_test, label = y_test)
```

### Set parameters for the xgboost model
```{r}
params <- list(
  objective = "multi:softmax",
  num_class = length(unique(y_train)),
  max_depth = 6,
  eta = 0.3
)
```

### Model training
```{r xgboost_train}
model <- xgb.train(params, dtrain, nrounds = 100)

# Make predictions on the test data
predictions <- predict(model, dtest)
```

### Model evaluation
```{r xgboost_eval}
accuracy <- sum(predictions == y_test) / length(y_test)
print(paste("Accuracy:", accuracy))
```
We get an accuracy of 87.42%, which looks pretty good.


## Individual Emotion Prediction Accuracy Analysis

### Convert the predictions and actual values to factors
```{r}
predictions_factor <- as.factor(predictions)
y_test_factor <- as.factor(y_test)
```

### Confusion matrix
```{r confusion_matrix}
cm <- confusionMatrix(predictions_factor, y_test_factor)

# Print the confusion matrix
print(cm)
```

### Precision, recall, and F1-score for each emotion
```{r}
precision <- cm$byClass[, "Precision"]
recall <- cm$byClass[, "Recall"]
F1 <- cm$byClass[, "F1"]

# Print precision, recall, and F1-score for each class
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-score:", F1))
```

From the confusion matrix and the statistics, we can infer the following:

**Overall Accuracy:** The overall accuracy of the model is *87.42%*, which means the model correctly predicted the emotion *87.42%* of the time.

**Class-wise Performance:**
1. **Sadness (0):** The model has a high precision (*92.11%*) and recall (*90.69%*) for predicting *sadness*, indicating that it is performing well for this class.

2. **Joy (1):** The model also performs well in predicting *joy* with a precision of *89.06%* and recall of *89.72%*.

3. **Love (2):** The precision and recall drop for *love* to *75.58%* and *76.37%* respectively, indicating that the model has some difficulty in correctly predicting this emotion.

4. **Anger (3):** The model performs well in predicting *anger* with a precision of *90.22%* and recall of *84.07%*.

5. **Fear (4):** The model has a good precision (*85.79%*) and recall (*82.98%*) for predicting *fear*.

6. **Surprise (5):** The precision drops significantly for *surprise* to *65.88%*, but the recall is high at *91.95%*. This indicates that while the model is able to capture most of the *surprise* instances, it also incorrectly classifies other emotions as *surprise*.

**Balanced Accuracy:** The balanced accuracy for each class is quite high (ranging from *87.07%* to *95.09%*), indicating that the model performs well across all classes, not just the majority class.

**F1-score:** The F1-score, which is the harmonic mean of precision and recall, gives us a single metric to evaluate the model. The F1-scores for the different classes range from *75.98%* (*love*) to *91.39%* (*sadness*), indicating that the model performs best for *sadness* and worst for *love*.


